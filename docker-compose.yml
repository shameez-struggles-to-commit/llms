services:
  llms:
    image: ghcr.io/servicestack/llms:latest
    container_name: llms-py
    ports:
      - "8000:8000"
    environment:
      # API Keys - Set these in your .env file or pass them directly
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GOOGLE_FREE_API_KEY=${GOOGLE_FREE_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - CODESTRAL_API_KEY=${CODESTRAL_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GROK_API_KEY=${GROK_API_KEY:-}
      - DASHSCOPE_API_KEY=${DASHSCOPE_API_KEY:-}
      - ZAI_API_KEY=${ZAI_API_KEY:-}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - AIREFINERY_API_KEY=${AIREFINERY_API_KEY:-}
      - VERBOSE=1
    volumes:
      # Maintain user-modifiable config files outside the container
      # llms.json and ui.json will be auto-created on first run
      - ~/.llms:/home/llms/.llms
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
